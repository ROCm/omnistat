\documentclass[11pt]{article}

\usepackage[margin=0.85in]{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{longtable}

\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
\renewcommand{\arraystretch}{1.12}
\setlength{\tabcolsep}{4pt}
\setlength{\LTpre}{0pt}
\setlength{\LTpost}{0pt}

\newcommand{\ctr}[1]{\texttt{#1}}

\title{Frontier Cassini Telemetry: Derived Interconnect Metrics (CXI)}
\author{Oscar Hernandez}
\date{\today}

\begin{document}
\maketitle

\section*{Conventions}
Two samples at times $t_1$ and $t_2$.
\begin{itemize}
  \item $\Delta X = X(t_2) - X(t_1)$
  \item $\Delta T = t_2 - t_1$
  \item Use a small $\epsilon$ for numerical stability when needed.
\end{itemize}

% Column widths sum to 1.00\textwidth
% Metric 0.14, Formula 0.22, Counters 0.28, Meaning 0.22, Mapping 0.14

\section*{Category 1: Throughput and Traffic Mix}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Throughput and traffic mix metrics derived from Cassini telemetry.}\label{tab:cat1}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

TX bandwidth &
$\Delta\,\ctr{hni\_sts\_tx\_ok\_octets}/\Delta T$ &
\ctr{hni\_sts\_tx\_ok\_octets} counts total bytes successfully transmitted on the Slingshot fabric &
How fast the node injects bytes into the network &
AI: sustained high in allreduce. MPI: bursty per timestep. Analytics: spikes in shuffle. \\

RX bandwidth &
$\Delta\,\ctr{hni\_sts\_rx\_ok\_octets}/\Delta T$ &
\ctr{hni\_sts\_rx\_ok\_octets} counts total bytes successfully received from the fabric &
How fast the node receives bytes from the network &
AI: receiving gradients or parameters. MPI: leaf roles. Analytics: receiving partitions. \\

Bidirectional bandwidth &
$\left(\Delta\,\ctr{hni\_sts\_tx\_ok\_octets}+\Delta\,\ctr{hni\_sts\_rx\_ok\_octets}\right)/\Delta T$ &
TX and RX OK octet counters above &
Total fabric traffic rate on the node &
AI: high in synchronized steps. MPI: high in exchange phases. Analytics: high in repartition. \\

TX to RX balance &
$\left(\Delta\,\ctr{hni\_sts\_tx\_ok\_octets}/\Delta T\right)\Big/\max\!\left(\Delta\,\ctr{hni\_sts\_rx\_ok\_octets}/\Delta T,\epsilon\right)$ &
TX and RX OK octet counters above &
Sender heavy versus receiver heavy behavior &
AI: parameter server or roots show imbalance. MPI: collective roots vs leaves. Analytics: reducers high RX. \\

Multicast TX share &
$\Delta\,\ctr{hni\_sts\_tx\_octets\_multi}/\max(\Delta\,\ctr{hni\_sts\_tx\_ok\_octets},1)$ &
\ctr{hni\_sts\_tx\_octets\_multi} counts bytes transmitted as fabric multicast; \ctr{hni\_sts\_tx\_ok\_octets} counts all TX bytes &
Fraction of TX bytes using multicast capability &
AI and MPI collectives: higher often indicates efficient collective use. Analytics: typically low. \\

IEEE TX share &
$\Delta\,\ctr{hni\_sts\_tx\_octets\_ieee}/\max(\Delta\,\ctr{hni\_sts\_tx\_ok\_octets},1)$ &
\ctr{hni\_sts\_tx\_octets\_ieee} counts bytes sent using IEEE Ethernet framing &
Fraction of TX bytes using standard unicast framing &
MPI point to point and analytics often higher. \\

Optimized protocol TX share &
$\Delta\,\ctr{hni\_sts\_tx\_octets\_opt}/\max(\Delta\,\ctr{hni\_sts\_tx\_ok\_octets},1)$ &
\ctr{hni\_sts\_tx\_octets\_opt} counts bytes sent on Slingshot optimized protocol paths &
Use of high performance NIC protocol paths &
AI tuned collectives and tuned MPI may be higher. \\

\end{longtable}
}

\section*{Category 2: Message Rate and Packet Size Efficiency}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Packet rate and packet size efficiency metrics.}\label{tab:cat2}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Packet send rate &
$\Delta\left(\sum_{tc=0}^{7}\ctr{hni\_pkts\_sent\_by\_tc\_tc}\right)/\Delta T$ &
\ctr{hni\_pkts\_sent\_by\_tc\_0..7} count packets sent per traffic class &
Packets per second injected into the fabric &
MPI fine grain: high. AI: high if chunking is too small. Analytics: varies. \\

Packet receive rate &
$\Delta\left(\sum_{tc=0}^{7}\ctr{hni\_pkts\_recv\_by\_tc\_tc}\right)/\Delta T$ &
\ctr{hni\_pkts\_recv\_by\_tc\_0..7} count packets received per traffic class &
Packets per second received &
Detect receiver pressure and skew. \\

Average bytes per TX packet &
$\Delta\,\ctr{hni\_sts\_tx\_ok\_octets}/\max\!\left(\Delta\sum_{tc=0}^{7}\ctr{hni\_pkts\_sent\_by\_tc\_tc},1\right)$ &
TX bytes divided by TX packet counts &
Efficiency proxy for message size on the wire &
Low often means overhead and latency bound behavior. \\

Small packet fraction TX &
$\Delta(\ctr{hni\_tx\_ok\_64}+\ctr{hni\_tx\_ok\_65\_to\_127}+\ctr{hni\_tx\_ok\_128\_to\_255})/\max\!\left(\Delta\sum_{tc=0}^{7}\ctr{hni\_pkts\_sent\_by\_tc\_tc},1\right)$ &
\ctr{hni\_tx\_ok\_*} size buckets count TX packets by size; \ctr{hni\_pkts\_sent\_by\_tc\_*} count total TX packets &
How dominated traffic is by very small packets &
MPI: latency pressure. AI: poor fusion or too many small collectives. Analytics: usually lower. \\

Large packet fraction TX &
$\Delta(\ctr{hni\_tx\_ok\_1024\_to\_2047}+\ctr{hni\_tx\_ok\_2048\_to\_4095}+\ctr{hni\_tx\_ok\_4096\_to\_8191}+\ctr{hni\_tx\_ok\_8192\_to\_max})/\max\!\left(\Delta\sum_{tc=0}^{7}\ctr{hni\_pkts\_sent\_by\_tc\_tc},1\right)$ &
Large TX size bucket counters count TX packets in large size ranges &
Whether packetization favors bandwidth efficiency &
AI and bandwidth bound phases prefer higher. Analytics large moves prefer higher. \\

\end{longtable}
}

\section*{Category 3: Link Utilization, Stalls, and Pipeline Blockage}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Link utilization and stall metrics.}\label{tab:cat3}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Link busy fraction &
$\Delta\,\ctr{cq\_cq\_oxe\_num\_flits}/\max\!\left(\Delta\,\ctr{cq\_cq\_oxe\_num\_flits}+\Delta\,\ctr{cq\_cq\_oxe\_num\_idles},1\right)$ &
\ctr{cq\_cq\_oxe\_num\_flits} counts TX link data units; \ctr{cq\_cq\_oxe\_num\_idles} counts idle cycles &
How continuously the link pushes data &
AI: sustained high. MPI: bursty. Analytics: spikes in shuffle. \\

Link stall per flit &
$\Delta\,\ctr{cq\_cq\_oxe\_num\_stalls}/\max\!\left(\Delta\,\ctr{cq\_cq\_oxe\_num\_flits},1\right)$ &
\ctr{cq\_cq\_oxe\_num\_stalls} counts TX stalls due to flow control or lack of credits &
Backpressure or congestion in TX pipeline &
Explains bandwidth plateaus under load. \\

CQ blocked cycles rate &
$\Delta(\ctr{cq\_cycles\_blocked\_0}+\ctr{cq\_cycles\_blocked\_1}+\ctr{cq\_cycles\_blocked\_2}+\ctr{cq\_cycles\_blocked\_3})/\Delta T$ &
\ctr{cq\_cycles\_blocked\_0..3} count blocked cycles in CQ related paths &
Time view of internal blocked progress &
Helps separate internal NIC blockage from fabric congestion. \\

NIC no work rate &
$\Delta\,\ctr{oxe\_channel\_idle}/\Delta T$ &
\ctr{oxe\_channel\_idle} counts cycles where NIC had nothing to send &
Whether the application feeds the NIC continuously &
Workflows: high between stages. MPI: high indicates serialization. AI: ideally low during comm phases. \\

\end{longtable}
}

\section*{Category 4: Congestion Control, ECN, and Flow Control}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Congestion control and flow control metrics.}\label{tab:cat4}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Pause received rate &
$\Delta\left(\sum_{i=0}^{7}\ctr{hni\_pause\_recv\_i}\right)/\Delta T$ &
\ctr{hni\_pause\_recv\_0..7} count pause events received &
Fabric or peers telling this node to slow down &
AI: spikes in synchronized collectives. MPI: hotspots. Analytics: skewed bursts. \\

Pause sent rate &
$\Delta\,\ctr{hni\_pause\_sent}/\Delta T$ &
\ctr{hni\_pause\_sent} counts pause events sent by this NIC &
This node throttling peers &
Often indicates local buffering pressure or receiver overload. \\

XOFF sent rate &
$\Delta\left(\sum_{i=0}^{7}\ctr{hni\_pause\_xoff\_sent\_i}\right)/\Delta T$ &
\ctr{hni\_pause\_xoff\_sent\_0..7} count XOFF events sent by class &
Strong signal that backpressure is being applied &
Often rises with heavy all to all and oversubscription. \\

ECN marking ratio request side &
$\Delta\left(\sum_{i=0}^{7}\ctr{ixe\_tc\_req\_ecn\_pkts\_i}\right)\Big/\max\!\left(\Delta\left(\sum_{i=0}^{7}\ctr{ixe\_tc\_req\_ecn\_pkts\_i}+\sum_{i=0}^{7}\ctr{ixe\_tc\_req\_no\_ecn\_pkts\_i}\right),1\right)$ &
\ctr{ixe\_tc\_req\_ecn\_pkts\_*} count ECN marked request packets; \ctr{ixe\_tc\_req\_no\_ecn\_pkts\_*} count unmarked request packets &
Congestion intensity seen by request traffic &
AI: can spike at scale. MPI: sustained ECN suggests topology stress. Analytics: peaks under skew. \\

ECN marking ratio response side &
$\Delta\left(\sum_{i=0}^{7}\ctr{ixe\_tc\_rsp\_ecn\_pkts\_i}\right)\Big/\max\!\left(\Delta\left(\sum_{i=0}^{7}\ctr{ixe\_tc\_rsp\_ecn\_pkts\_i}+\sum_{i=0}^{7}\ctr{ixe\_tc\_rsp\_no\_ecn\_pkts\_i}\right),1\right)$ &
\ctr{ixe\_tc\_rsp\_ecn\_pkts\_*} count ECN marked response packets; \ctr{ixe\_tc\_rsp\_no\_ecn\_pkts\_*} count unmarked response packets &
Congestion intensity seen by response traffic &
Helps when responses are delayed more than requests. \\

Congestion discard rate &
$\Delta\left(\sum_{i=0}^{7}\ctr{hni\_discard\_cntr\_i}+\ctr{hni\_fgfc\_discard}\right)/\Delta T$ &
\ctr{hni\_discard\_cntr\_0..7} count discards; \ctr{hni\_fgfc\_discard} counts discards tied to flow control &
Whether congestion causes drops &
All workloads: should be near zero, otherwise expect tail latency and retries. \\

\end{longtable}
}

\section*{Category 5: Credit Pressure, Resource Occupancy, and Queue Health}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Credit pressure and resource occupancy metrics.}\label{tab:cat5}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Command credits in use rate proxy &
$\Delta\,\ctr{cq\_sts\_credits\_in\_use\_lpe\_cmd\_credits}/\Delta T$ &
\ctr{cq\_sts\_credits\_in\_use\_lpe\_cmd\_credits} tracks credits in use for LPE command traffic &
Whether command pipelines saturate &
MPI and AI with many concurrent ops can drive this higher. \\

Receive FIFO credits in use rate proxy &
$\Delta\,\ctr{cq\_sts\_credits\_in\_use\_lpe\_rcv\_fifo\_credits}/\Delta T$ &
\ctr{cq\_sts\_credits\_in\_use\_lpe\_rcv\_fifo\_credits} tracks receive FIFO credit usage &
Receiver buffering pressure &
Analytics reducers and skewed receivers can drive this higher. \\

PI posted credits in use rate proxy &
$\Delta\,\ctr{parbs\_sts\_credits\_in\_use\_tarb\_pi\_posted\_credits}/\Delta T$ &
\ctr{parbs\_sts\_credits\_in\_use\_tarb\_pi\_posted\_credits} tracks PI posted credits usage &
Internal path pressure proxy &
Useful if stalls high but link busy low, suggesting internal bottleneck. \\

Resource busy rate &
$\Delta\,\ctr{pct\_resource\_busy}/\Delta T$ &
\ctr{pct\_resource\_busy} counts times PCT reports resource busy conditions &
Internal contention in NIC resources &
Often rises with high outstanding requests. \\

Endpoint table pressure proxy &
$\Delta\,\ctr{pct\_prf\_tct\_status\_tct\_in\_use}/\max\!\left(\Delta\,\ctr{pct\_prf\_tct\_status\_max\_tct\_in\_use},1\right)$ &
\ctr{pct\_prf\_tct\_status\_tct\_in\_use} tracks TCT entries in use; \ctr{pct\_prf\_tct\_status\_max\_tct\_in\_use} tracks max observed &
How close transaction tables get to saturation &
MPI many peers and workflow bursty connections can stress this. \\

\end{longtable}
}

\section*{Category 6: Dynamic Routing and Ordering Proxies}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Indirect routing and ordering indicators.}\label{tab:cat6}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Ordered versus unordered ratio &
$\Delta\,\ctr{oxe\_ioi\_pkts\_ordered}/\max\!\left(\Delta\,\ctr{oxe\_ioi\_pkts\_unordered},1\right)$ &
\ctr{oxe\_ioi\_pkts\_ordered} counts ordered mode packets; \ctr{oxe\_ioi\_pkts\_unordered} counts unordered mode packets &
Ordering constraints versus flexible handling &
MPI protocols requiring strict ordering often push ordered higher. \\

Unordered fraction &
$\Delta\,\ctr{oxe\_ioi\_pkts\_unordered}/\max\!\left(\Delta(\ctr{oxe\_ioi\_pkts\_ordered}+\ctr{oxe\_ioi\_pkts\_unordered}),1\right)$ &
Same counters as above &
How much traffic uses unordered handling &
Higher can correlate with better load spreading depending on transport. \\

Ordered request fraction &
$\Delta\,\ctr{pct\_req\_ordered}/\max\!\left(\Delta(\ctr{pct\_req\_ordered}+\ctr{pct\_req\_unordered}),1\right)$ &
\ctr{pct\_req\_ordered} counts ordered requests; \ctr{pct\_req\_unordered} counts unordered requests &
Request level ordering tendency &
MPI sync heavy patterns can show higher ordered fraction. \\

\end{longtable}
}

\section*{Category 7: Latency, Tail Behavior, and Timeouts}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Latency and timeout metrics.}\label{tab:cat7}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Mean response latency proxy &
$\left(\sum_i \Delta\,\ctr{pct\_req\_rsp\_latency\_i}\,c_i\right)\big/\max\!\left(\sum_i \Delta\,\ctr{pct\_req\_rsp\_latency\_i},1\right)$ &
\ctr{pct\_req\_rsp\_latency\_0..N} count responses in latency bins; $c_i$ is bin center from documentation &
Typical response latency proxy &
MPI fine grain and AI sync steps are sensitive. Analytics shows tail effects. \\

Tail latency fraction &
$\left(\sum_{i\in H}\Delta\,\ctr{pct\_req\_rsp\_latency\_i}\right)\big/\max\!\left(\sum_i\Delta\,\ctr{pct\_req\_rsp\_latency\_i},1\right)$ &
Same latency histogram counters; $H$ is set of highest latency bins &
Fraction of very slow responses &
Predictor of stragglers and synchronization stalls. \\

Timeout rate &
$\Delta(\ctr{pct\_sct\_timeouts}+\ctr{pct\_tct\_timeouts}+\ctr{pct\_trs\_replay\_pend\_drops})/\Delta T$ &
\ctr{pct\_sct\_timeouts} counts session timeouts; \ctr{pct\_tct\_timeouts} counts transaction timeouts; \ctr{pct\_trs\_replay\_pend\_drops} counts replay pending drops &
Severe stress signal &
AI: instability at scale. MPI: catastrophic scaling loss. Analytics: shuffle stalls and retries. \\

\end{longtable}
}

\section*{Category 8: Correctness and Health, Errors and Corruption}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Correctness and link health signals.}\label{tab:cat8}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Bad TX octet rate &
$\Delta\,\ctr{hni\_sts\_tx\_bad\_octets}/\Delta T$ &
\ctr{hni\_sts\_tx\_bad\_octets} counts bad TX octets detected &
Physical or protocol issues on TX path &
All workloads: should be near zero. \\

Bad RX octet rate &
$\Delta\,\ctr{hni\_sts\_rx\_bad\_octets}/\Delta T$ &
\ctr{hni\_sts\_rx\_bad\_octets} counts bad RX octets detected &
Physical or protocol issues on RX path &
All workloads: should be near zero. \\

ECC corrected error rate &
$\Delta\,\ctr{hni\_pcs\_corrected\_cw}/\Delta T$ &
\ctr{hni\_pcs\_corrected\_cw} counts corrected codewords on the physical coding sublayer &
Underlying link reliability signal &
Usually low. Rising trends can predict instability. \\

ECC uncorrected error rate &
$\Delta\,\ctr{hni\_pcs\_uncorrected\_cw}/\Delta T$ &
\ctr{hni\_pcs\_uncorrected\_cw} counts uncorrectable codewords &
Severe link issues &
Any sustained non zero warrants attention. \\

\end{longtable}
}

\section*{Category 9: Compute Communication Overlap (Requires Phase Labeling)}
{\small
\begin{longtable}{@{}P{0.14\textwidth}P{0.22\textwidth}P{0.28\textwidth}P{0.22\textwidth}P{0.14\textwidth}@{}}
\caption{Overlap metrics require application phase boundaries from tracing or profiling.}\label{tab:cat9}\\
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endfirsthead
\toprule
Metric & Formula & Counters used and what each counter measures & What this metric tells you & Typical workload mapping \\
\midrule
\endhead
\bottomrule
\endfoot

Overlap score &
$\left(\sum_{k\in C}\overline{BW}_{bi,k}\,\Delta t_k\right)\big/\max\!\left(\sum_{k\in A}\overline{BW}_{bi,k}\,\Delta t_k,1\right)$ &
Bandwidth comes from TX and RX OK octets; compute intervals $C$ come from tracing or profiling; $A$ is all intervals &
How much network work happens while compute is active &
AI: should be high when tuned. MPI: often low unless nonblocking and pipelined. Analytics: depends on pipeline overlap. \\

\end{longtable}
}

\section*{Note on dynamic routing}
Cassini telemetry typically does not expose a single direct counter named dynamic routing enabled. Routing and load spreading are usually inferred using ordered versus unordered handling, congestion signals such as ECN and pause, and tail latency and timeouts.

\end{document}

